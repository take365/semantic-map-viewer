import os
import pickle
import pandas as pd
import numpy as np
from datetime import datetime
from llm import request_to_embed
from dotenv import load_dotenv

load_dotenv()

# --- è¨­å®š ---
SEARCH_QUERY = "æ—¥æœ¬ã®ãŠã„ã—ã„é£Ÿã¹ç‰©"
TOP_K = 10
FILTER_CATEGORIES = []  # ç©ºãƒªã‚¹ãƒˆã§ã€Œå…¨ã‚«ãƒ†ã‚´ãƒªã€
USE_CACHE = True

# --- ãƒ‘ã‚¹å®šç¾© ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(BASE_DIR, "data/embedded_items.pkl")
CACHE_PATH = os.path.join(BASE_DIR, "data/embed_cache.pkl")
LOG_PATH = os.path.join(BASE_DIR, f"search_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")

# --- ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ ---
if USE_CACHE and os.path.exists(CACHE_PATH):
    with open(CACHE_PATH, "rb") as f:
        embed_cache = pickle.load(f)
else:
    embed_cache = {}

# --- ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«å–å¾—ï¼ˆsmall + largeï¼‰ ---
if SEARCH_QUERY in embed_cache:
    query_vec_small, query_vec_large = embed_cache[SEARCH_QUERY]
else:
    query_vec_small = request_to_embed([SEARCH_QUERY], model="text-embedding-3-small")[0]
    query_vec_large = request_to_embed([SEARCH_QUERY], model="text-embedding-3-large")[0]
    embed_cache[SEARCH_QUERY] = (query_vec_small, query_vec_large)
    if USE_CACHE:
        with open(CACHE_PATH, "wb") as f:
            pickle.dump(embed_cache, f)

# --- åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
with open(DATA_PATH, "rb") as f:
    df = pickle.load(f)

# --- é¡ä¼¼åº¦è¨ˆç®—é–¢æ•° ---
def cosine_similarity(vec1, vec2):
    v1 = np.array(vec1)
    v2 = np.array(vec2)
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

# --- ã‚«ãƒ†ã‚´ãƒªã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚ã‚Œã°ï¼‰ ---
if FILTER_CATEGORIES:
    df = df[df["ã‚«ãƒ†ã‚´ãƒª"].isin(FILTER_CATEGORIES)]

# --- é¡ä¼¼åº¦è¨ˆç®— ---
df["ä¸€è‡´ç‡_small"] = df["small"].apply(lambda vec: cosine_similarity(query_vec_small, vec))
df["ä¸€è‡´ç‡_large"] = df["large"].apply(lambda vec: cosine_similarity(query_vec_large, vec))

# --- å‡ºåŠ›æ•´å½¢ ---
df["ä¸€è‡´ç‡_small"] = (df["ä¸€è‡´ç‡_small"] * 100).round(1)
df["ä¸€è‡´ç‡_large"] = (df["ä¸€è‡´ç‡_large"] * 100).round(1)

# --- smallã§ã‚½ãƒ¼ãƒˆã—ãŸä¸Šä½Kä»¶ ---
results_small = df.sort_values("ä¸€è‡´ç‡_small", ascending=False).head(TOP_K).copy()

# --- largeã§ã‚½ãƒ¼ãƒˆã—ãŸä¸Šä½Kä»¶ ---
results_large = df.sort_values("ä¸€è‡´ç‡_large", ascending=False).head(TOP_K).copy()

# --- ãƒ­ã‚°å‡ºåŠ› ---
with open(LOG_PATH, "w", encoding="utf-8") as f:
    f.write(f"ğŸ” æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰: {SEARCH_QUERY}\n")
    f.write(f"ğŸ¯ å¯¾è±¡ã‚«ãƒ†ã‚´ãƒª: {'å…¨ã¦' if not FILTER_CATEGORIES else FILTER_CATEGORIES}\n")
    
    f.write(f"\n   small\n\n")
    f.write(f"{'ã‚«ãƒ†ã‚´ãƒª':<8} {'å†…å®¹':<14} {'small(%)':>10}\n")
    f.write("-" * 42 + "\n")
    for _, row in results_small.iterrows():
        f.write(f"{row['ã‚«ãƒ†ã‚´ãƒª']:<8} {row['å†…å®¹']:<14} {row['ä¸€è‡´ç‡_small']:>10.1f}\n")

    f.write(f"\n   large\n\n")
    f.write(f"{'ã‚«ãƒ†ã‚´ãƒª':<8} {'å†…å®¹':<14} {'large(%)':>10}\n")
    f.write("-" * 42 + "\n")
    for _, row in results_large.iterrows():
        f.write(f"{row['ã‚«ãƒ†ã‚´ãƒª']:<8} {row['å†…å®¹']:<14} {row['ä¸€è‡´ç‡_large']:>10.1f}\n")

print(f"âœ… çµæœã‚’ {LOG_PATH} ã«å‡ºåŠ›ã—ã¾ã—ãŸã€‚")
